# training: python train_DDAffinity.py ./configs/train/PPIformer.yml --tag PPIFORMER --device cuda:0
# test: python test_DDAffinity.py ./configs/train/PPIformer.yml  --device cuda:0
checkpoints:
  - ./trained_models/PPIFORMER.pt
early_stoppingdir: ./early_stopping
model:
  encoder:
    node_feat_dim: 128
    edge_feat_dim: 128
    num_layers: 3
  hidden_dim: 128
  k1: 20    # knn neighbors
  k2: 7     # long range
  k3: 3     # neighbors of sequence
  dropout: 0.1
  noise_bb: &noise_bb 0.15
  noise_sd: &noise_sd 0.25
  num_labels: 6

data:
    csv_path: ./data/SKEMPI2/skempi_v2_cache/skempi_v2.csv
    pdb_wt_dir: ./data/SKEMPI2/skempi_v2_cache/wildtype
    pdb_mt_dir: ./data/SKEMPI2/skempi_v2_cache/optimized
    prior_dir: ./data/SKEMPI2/skempi_v2_cache
    cache_dir: ./data/SKEMPI2/skempi_v2_cache/entries_cache
    train:
      transform:
        # Only backbone atoms and CB are visible to rotamer predictor
        - type: select_atom
          resolution: backbone+CB
        - type: add_atom_noise
          noise_backbone: *noise_bb
          noise_sidechain: *noise_sd
        - type: selected_region_fixed_size_patch
          select_attr: mut_flag
          patch_size: 256
    val:
      transform:
        - type: select_atom
          resolution: backbone+CB
        - type: add_atom_noise
          noise_backbone: 0.0
          noise_sidechain: 0.0
        - type: selected_region_fixed_size_patch
          select_attr: mut_flag
          patch_size: 256
    is_single: 2          # 0:single,1:multiple,2:overall
    cath_fold: False      # default: False
    PPIformer: True       # True
    GearBind: False

train:
  loss_weights:
    loss_mse: 1.0
    loss_foldx: 1.0
    loss_cath: 1.0
  max_epochs: 150
  early_stopping_epoch: 100
  val_freq: 5
  batch_size: 32
  seed: 2024
  num_cvfolds: 3
  max_grad_norm: 100.0
  optimizer:
    type: adam
    lr: &lr 5.e-4     #  lr_max
    lr_2: &lr_2 5.e-6   #  lr_max_esm2
    lr_3: &lr_3 5.e-5   #  lr_max_foldx
    weight_decay: 1.e-4
    weight_decay_2: 5.e-6
    weight_decay_3: 5.e-6
    beta1: 0.9
    beta2: 0.999
  scheduler:
    type: lambdaLR
    warm_up_iters: 4000
    T_iters: 5000  # Cycle
    lr_max: *lr
    lr_min: 2.e-4
    lr_2_max: *lr_2
    lr_2_min: 2.e-6
    lr_3_max: *lr_3
    lr_3_min: 2.e-5